{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post Estimation using Tensorflow.js \n",
    "\n",
    "![alt text](https://cdn-images-1.medium.com/max/1200/1*BKZqEPtvM-6xwarhABZQnA.gif \"Logo Title Text 1\")\n",
    "\n",
    "- ML used to require lots of time and money to get started \n",
    "- Configurations, dependencies, hardware costs, lots of headaches\n",
    "- But now anyone can train and test ML models in the browser really easily using Tensorflow.js\n",
    "- Even python was more difficult (jupyter notebooks, numpy, scikit, pandas, etc)\n",
    "- ML in the browser means\n",
    "##### Privacy - Data is local, none leaves the clients device. Much safer.\n",
    "##### Wide distribution -  JavaScript has one of the widest install bases of any language and framework. \n",
    "##### Distributed Computing - Leverage client side data from many users to help train a model\n",
    "\n",
    "![alt text](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/04/0V4HYbZt28PHZZ3aD.png \"Logo Title Text 1\")\n",
    "\n",
    "## What does the pipeline look like? \n",
    "\n",
    "###### Example - Using a pretrained model for classification (3 Steps) \n",
    "\n",
    "#### Step 1 - Load Model\n",
    "\n",
    "- First, we'll need to import two files that define the structure of the model (model file) & its trained weights (weights manifest)\n",
    "\n",
    "![alt text](https://cdn-images-1.medium.com/max/1600/1*m_QUz768MxAK4KXnvVL2nQ.jpeg \"Logo Title Text 1\")\n",
    "`\n",
    "import * as tdc from '@tensorflow/tfjs-core';\n",
    "import { loadFrozenModel } from '@tensorflow/tfjs-converter';\n",
    "import { IMAGENET_CLASSES } from './imagenet_classes';\n",
    "const MODEL_URL = '/models/mobilenet/optimized_model.pb';\n",
    "const WEIGHTS_URL = '/models/mobilenet/weights_manifest.json';\n",
    "const INPUT_NODE_NAME = 'input';\n",
    "const OUTPUT_NODE_NAME = 'MobilenetV1/Predictions/Reshape_1';\n",
    "const PREPROCESS_DIVIDOR = tfc.scalar(255 / 2);\n",
    "export defualt class MobileNet {\n",
    "async load () {\n",
    "this.model = await loadFrozenModel(MODEL_URL, WEIGHTS_URL);\n",
    "`\n",
    "\n",
    "#### Step 2 - Preprocessing\n",
    "\n",
    "\n",
    "- A neural network has a specific input definition so we'll need to do some preprocessing in order to get your input into the right shape\n",
    "- First, we'll convert pixels to TensorFlow.js input tensor\n",
    "- Then, we'll crop the input if you want to use parts of the image\n",
    "- Lastly, we'll set batch input dimensions to 0, since you only want to infer one image\n",
    "\n",
    "`\n",
    "preprocess(source) {\n",
    "console.log('input size:' this.model.input.shape); // [224,224,3]\n",
    "// memory enhancements - tells the system to throw away this tensor after usage\n",
    "return tf.tidy(() => {\n",
    "const input = tfc.fromPixels(source);\n",
    "// crop the image to match the input size of mobilenet\n",
    "// this is 224x224 px with 3 channel (RGB) color data\n",
    "// get a square from the middle of the image\n",
    "// resizing is done by the html5 canvas\n",
    "const croppedImage = MobileNet.cropImage(input);\n",
    "//mobilenet expects a batched input - build a [1, 224, 224, 3] tensor\n",
    "const bachtedImage = croppedImage.expandDims(0);\n",
    "//normalization of the pixel color channel values\n",
    "//instead of 0-225 we get values between -1 and 1\n",
    "return batchedImage.toFloat().div(tf.scalar(127)).sub(tf.scalar(1));\n",
    "});\n",
    "}\n",
    "`\n",
    "\n",
    "#### Step 3 - Inference\n",
    "\n",
    "- The inference of an input aka a class prediction is just two lines of code\n",
    "\n",
    "`\n",
    "predict(source) {\n",
    "const processedInput = this.preprocess(source);\n",
    "return this.model.predict(processedInput);\n",
    "}\n",
    "`\n",
    "- The result (prediction) will be a dictionary containing the probabilities for each class. \n",
    "- The max value within the dictionary is the most likely class\n",
    "\n",
    "##### Good resources\n",
    "\n",
    "- JS docs https://js.tensorflow.org/tutorials/core-concepts.html \n",
    "- My video https://www.youtube.com/watch?v=Nc8kZABv-KE\n",
    "- Shiffmans series https://www.youtube.com/watch?v=Qt3ZABW5lD0 \n",
    "\n",
    "### What is pose estimation?\n",
    "\n",
    "![alt text](http://www.ee.cuhk.edu.hk/~wyang/images/pose-cvpr2016.jpg \"Logo Title Text 1\")\n",
    "\n",
    "- Its a computer vision techniques that detect human figures in images and video\n",
    "- The algorithm is simply estimating where key body joints are, not 'who' is in the image\n",
    "- This has many uses \n",
    "##### interactive installations that react to the body\n",
    "##### Augmented reality\n",
    "##### animation\n",
    "##### fitness\n",
    "\n",
    "![alt text](https://cse.sc.edu/~fan23/projects/cvpr15/overview.png \"Logo Title Text 1\")\n",
    "\n",
    "- Usually open source pose detection systems all require specialized hardware and/or cameras, as well as quite a bit of system setup. \n",
    "- With PoseNet running on TensorFlow.js anyone with a decent webcam-equipped desktop or phone can experience it in the browser\n",
    "- Javascript developers can tinker and use this technology with just a few lines of code\n",
    "\n",
    "### PoseNet\n",
    "\n",
    "- PoseNet can be used to estimate either a single pose or multiple poses\n",
    "- The single person pose detector is faster and simpler but requires only one subject present in the image \n",
    "- At a high level pose estimation happens in two phases\n",
    "- First, An input RGB image is fed through a convolutional neural network.\n",
    "- Either a single-pose or multi-pose decoding algorithm is used to decode poses, pose confidence scores, keypoint positions, and keypoint confidence scores from the model outputs\n",
    "- A Pose — at the highest level, PoseNet will return a pose object that contains a list of keypoints and an instance-level confidence score for each detected person.\n",
    "\n",
    "![alt text](https://cdn-images-1.medium.com/max/1600/1*3bg3CO1b4yxqgrjsGaSwBw.png \"Logo Title Text 1\")\n",
    "\n",
    "- A Pose confidence score determines the overall confidence in the estimation of a pose\n",
    "- It ranges between 0.0 and 1.0\n",
    "- It can be used to hide poses that are not deemed strong enough.\n",
    "- A Keypoint is a part of a person’s pose that is estimated, such as the nose, right ear, left knee, right foot, etc. \n",
    "- It contains both a position and a keypoint confidence score. \n",
    "- PoseNet currently detects 17 keypoints illustrated in the following diagram:\n",
    "\n",
    "![alt text](https://cdn-images-1.medium.com/max/1600/1*7qDyLpIT-3s4ylULsrnz8A.png \"Logo Title Text 1\")\n",
    "\n",
    "-  AKeypoint Confidence Score determines the confidence that an estimated keypoint position is accurate. \n",
    "- It ranges between 0.0 and 1.0. It can be used to hide keypoints that are not deemed strong enough.\n",
    "- A Keypoint Position is 2D x and y coordinates in the original input image where a keypoint has been detected.\n",
    "\n",
    "![alt text](https://cdn-images-1.medium.com/max/1600/1*SpWPwprVuNYhXs44iTSODg.png \"Logo Title Text 1\")\n",
    "\n",
    "### Single Person Pose Estimation\n",
    "\n",
    "- single-pose estimation algorithm is the simpler and faster of the two\n",
    "- Its ideal use case is for when there is only one person centered in an input image or video\n",
    "- the inputs for the single-pose estimation algorithm are as followss\n",
    "\n",
    "- Input image element — An html element that contains an image to predict poses for, such as a video or image tag. \n",
    "- Image scale factor — What to scale the image by before feeding it through the network. \n",
    "- Flip horizontal — If the poses should be flipped/mirrored horizontally\n",
    "- Output stride — This parameter affects the height and width of the layers in the neural network\n",
    "\n",
    "- The outputs are as follows:\n",
    "\n",
    "- A pose, containing both a pose confidence score and an array of 17 keypoints.\n",
    "- Each keypoint contains a keypoint position and a keypoint confidence score.\n",
    "- All the keypoint positions have x and y coordinates in the input image space, and can be mapped directly onto the image\n",
    "\n",
    "- The PoseNet model is image size invariant, which means it can predict pose positions in the same scale as the original image regardless of whether the image is downscaled. \n",
    "-  The output stride determines how much we’re scaling down the output relative to the input image size. \n",
    "\n",
    "![alt text](https://cdn-images-1.medium.com/max/1600/1*zXXwR16kprAWLPIOKCrXLw.png \"Logo Title Text 1\")\n",
    "\n",
    "- When the output stride is set to 8 or 16, the amount of input striding in the layers is reduced to create a larger output resolution\n",
    "\n",
    "\n",
    "- When PoseNet processes an image, what is in fact returned is a heatmap along with offset vectors that can be decoded to find high confidence areas in the image that correspond to pose keypoints\n",
    "\n",
    "![alt text](https://cdn-images-1.medium.com/max/1600/1*mcaovEoLBt_Aj0lwv1-xtA.png \"Logo Title Text 1\")\n",
    "\n",
    "- Both of these outputs are 3D tensors with a height and width that we’ll refer to as the resolution. \n",
    "- Each heatmap is a 3D tensor of size resolution x resolution x 17, since 17 is the number of keypoints detected by PoseNe\n",
    "- Each offset vector is a 3D tensor of size resolution x resolution x 34, where 34 is the number of keypoints * 2. \n",
    "\n",
    "- After the image is fed through the model, we perform a few calculations to estimate the pose from the outputs. \n",
    "- The single-pose estimation algorithm for example returns a pose confidence score which itself contains an array of keypoints (indexed by part ID) each with a confidence score and x, y position.\n",
    "- To get the keypoints of the pose, A sigmoid activation is done on the heatmap to get the scores.\n",
    "`\n",
    "scores = heatmap.sigmoid()\n",
    "argmax2d\n",
    "`\n",
    "\n",
    "- This is done on the keypoint confidence scores to get the x and y index in the heatmap with the highest score for each part, which is essentially where the part is most likely to exist. This produces a tensor of size 17x2, with each row being the y and x index in the heatmap with the highest score for each part.\n",
    "\n",
    "`\n",
    "heatmapPositions = scores.argmax(y, x)\n",
    "`\n",
    "\n",
    "- The offset vector for each part is retrieved by getting the x and y from the offsets corresponding to the x and y index in the heatmap for that part. \n",
    "\n",
    "`\n",
    "offsetVector = [offsets.get(y, x, k), offsets.get(y, x, 17 + k)]\n",
    "`\n",
    "\n",
    "\n",
    "- To get the keypoint, each part’s heatmap x and y are multiplied by the output stride then added to their corresponding offset vector, which is in the same scale as the original image.\n",
    "\n",
    "`\n",
    "keypointPositions = heatmapPositions * outputStride + offsetVectors\n",
    "`\n",
    "\n",
    "- Finally, each keypoint confidence score is the confidence score of its heatmap position. The pose confidence score is the mean of the scores of the keypoints.\n",
    "\n",
    "### Multi person pose estimation\n",
    "\n",
    "![alt text](https://cdn-images-1.medium.com/max/1600/1*EZOqbMLkIwBgyxrKLuQTHA.png \"Logo Title Text 1\")\n",
    "\n",
    "- Slower, but if multiple people appear in a picture, their detected keypoints are less likely to be associated with the wrong pose. \n",
    "-  Moreover, an attractive property of this algorithm is that performance is not affected by the number of persons in the input image. Whether there are 15 persons to detect or 5, the computation time will be the same.\n",
    "\n",
    "Let’s review the inputs:\n",
    "\n",
    "- Input image element — Same as single-pose estimation\n",
    "- Image scale factor — Same as single-pose estimation\n",
    "- Flip horizontal — Same as single-pose estimation\n",
    "- Output stride — Same as single-pose estimation\n",
    "- Maximum pose detections — An integer. Defaults to 5. The maximum number of poses to detect.\n",
    "- Pose confidence score threshold — 0.0 to 1.0. Defaults to 0.5. At a high level, this controls the minimum confidence score of poses that are returned.\n",
    "- Non-maximum suppression (NMS) radius — A number in pixels. At a high level, this controls the minimum distance between poses that are returned. This value defaults to 20, which is probably fine for most cases. It should be increased/decreased as a way to filter out less accurate poses but only if tweaking the pose confidence score is not good enough.\n",
    "\n",
    "Let’s review the outputs:\n",
    "\n",
    "- A promise that resolves with an array of poses.\n",
    "- Each pose contains the same information as described in the single-person estimation algorithm.\n",
    "This short code block shows how to use the multi-pose estimation algorithm:\n",
    "\n",
    "`\n",
    "const imageScaleFactor = 0.50;\n",
    "const flipHorizontal = false;\n",
    "const outputStride = 16;\n",
    "// get up to 5 poses\n",
    "const maxPoseDetections = 5;\n",
    "// minimum confidence of the root part of a pose\n",
    "const scoreThreshold = 0.5;\n",
    "// minimum distance in pixels between the root parts of poses\n",
    "const nmsRadius = 20;\n",
    "const imageElement = document.getElementById('cat');\n",
    "// load posenet\n",
    "const net = await posenet.load();\n",
    "const poses = await net.estimateMultiplePoses(\n",
    "  imageElement, imageScaleFactor, flipHorizontal, outputStride,    \n",
    "  maxPoseDetections, scoreThreshold, nmsRadius);\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
